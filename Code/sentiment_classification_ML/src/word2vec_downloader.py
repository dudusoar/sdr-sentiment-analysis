# coding: utf-8
'''
filename: word2vec_downloader.py
function: Word2Vecæ¨¡å‹ä¸‹è½½å’Œç®¡ç†
'''

import os
import gdown
import requests
from urllib.parse import urlparse
from gensim.models import KeyedVectors
import zipfile

class Word2VecDownloader:
    """Word2Vecæ¨¡å‹ä¸‹è½½å™¨"""
    
    def __init__(self, model_dir='data'):
        self.model_dir = model_dir
        self.model_path = os.path.join(model_dir, 'GoogleNews-vectors-negative300.bin')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
    
    def check_model_exists(self):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return os.path.exists(self.model_path)
    
    def verify_model_integrity(self):
        """éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        if not self.check_model_exists():
            return False
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å° (GoogleNewsæ¨¡å‹åº”è¯¥çº¦ä¸º1.6GB)
            file_size = os.path.getsize(self.model_path)
            if file_size < 1000000000:  # å°äº1GBè¯´æ˜ä¸‹è½½ä¸å®Œæ•´
                print(f"æ¨¡å‹æ–‡ä»¶å¤§å°å¼‚å¸¸: {file_size} bytes (æœŸæœ› > 1GB)")
                return False
            
            # å°è¯•è¯»å–æ–‡ä»¶å¤´éƒ¨
            with open(self.model_path, 'rb') as f:
                header = f.read(100)
                if len(header) < 10:
                    print("æ¨¡å‹æ–‡ä»¶å¤´éƒ¨è¯»å–å¤±è´¥")
                    return False
            
            print(f"æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡: {file_size} bytes")
            return True
            
        except Exception as e:
            print(f"éªŒè¯æ¨¡å‹å®Œæ•´æ€§å¤±è´¥: {e}")
            return False
    
    def fix_encoding_issues(self):
        """ä¿®å¤ç¼–ç é—®é¢˜"""
        print("å°è¯•ä¿®å¤Word2Vecç¼–ç é—®é¢˜...")
        
        # æ–¹æ³•1: ä½¿ç”¨ä¸åŒçš„ç¼–ç å‚æ•°åŠ è½½
        encoding_methods = [
            {'binary': True, 'unicode_errors': 'ignore'},
            {'binary': True, 'encoding': 'utf-8', 'unicode_errors': 'ignore'},
            {'binary': True, 'encoding': 'latin-1'},
            {'binary': False, 'encoding': 'utf-8', 'unicode_errors': 'ignore'},
        ]
        
        for i, params in enumerate(encoding_methods):
            try:
                print(f"å°è¯•åŠ è½½æ–¹æ³• {i+1}: {params}")
                model = KeyedVectors.load_word2vec_format(self.model_path, **params)
                print(f"âœ“ åŠ è½½æˆåŠŸ! è¯æ±‡é‡: {len(model.key_to_index)}")
                return model
            except Exception as e:
                print(f"âœ— æ–¹æ³• {i+1} å¤±è´¥: {e}")
                continue
        
        return None
    
    def backup_and_redownload(self):
        """å¤‡ä»½æŸåæ–‡ä»¶å¹¶é‡æ–°ä¸‹è½½"""
        if self.check_model_exists():
            # å¤‡ä»½æŸåçš„æ–‡ä»¶
            backup_path = self.model_path + '.corrupted'
            try:
                os.rename(self.model_path, backup_path)
                print(f"å·²å¤‡ä»½æŸåæ–‡ä»¶åˆ°: {backup_path}")
            except:
                os.remove(self.model_path)
                print("å·²åˆ é™¤æŸåæ–‡ä»¶")
        
        # é‡æ–°ä¸‹è½½
        print("å¼€å§‹é‡æ–°ä¸‹è½½Word2Vecæ¨¡å‹...")
        return self.download_model()

    def download_from_google_drive(self, file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM'):
        """ä»Google Driveä¸‹è½½Word2Vecæ¨¡å‹"""
        print("æ­£åœ¨ä»Google Driveä¸‹è½½Word2Vecæ¨¡å‹...")
        url = f'https://drive.google.com/uc?id={file_id}'
        
        try:
            # ä¸‹è½½æ–‡ä»¶
            output_path = self.model_path
            gdown.download(url, output_path, quiet=False)
            print(f"æ¨¡å‹ä¸‹è½½å®Œæˆ: {output_path}")
            return True
        except Exception as e:
            print(f"ä»Google Driveä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def download_from_alternative_source(self):
        """ä»å¤‡ç”¨æºä¸‹è½½Word2Vecæ¨¡å‹"""
        print("æ­£åœ¨ä»å¤‡ç”¨æºä¸‹è½½Word2Vecæ¨¡å‹...")
        
        # å¤‡ç”¨ä¸‹è½½é“¾æ¥ (è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æœ‰æ•ˆçš„é“¾æ¥)
        urls = [
            'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz',
            # å¯ä»¥æ·»åŠ æ›´å¤šå¤‡ç”¨é“¾æ¥
        ]
        
        for url in urls:
            try:
                print(f"å°è¯•ä» {url} ä¸‹è½½...")
                response = requests.get(url, stream=True)
                
                if response.status_code == 200:
                    # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œéœ€è¦è§£å‹
                    if url.endswith('.gz'):
                        import gzip
                        output_path = self.model_path + '.gz'
                        
                        with open(output_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        
                        # è§£å‹æ–‡ä»¶
                        with gzip.open(output_path, 'rb') as f_in:
                            with open(self.model_path, 'wb') as f_out:
                                f_out.write(f_in.read())
                        
                        # åˆ é™¤å‹ç¼©æ–‡ä»¶
                        os.remove(output_path)
                    else:
                        with open(self.model_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                    
                    print(f"æ¨¡å‹ä¸‹è½½å®Œæˆ: {self.model_path}")
                    return True
                    
            except Exception as e:
                print(f"ä» {url} ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        return False
    
    def download_smaller_model(self):
        """ä¸‹è½½è¾ƒå°çš„Word2Vecæ¨¡å‹ä½œä¸ºæ›¿ä»£"""
        print("æ­£åœ¨ä¸‹è½½è¾ƒå°çš„Word2Vecæ¨¡å‹...")
        
        try:
            import gensim.downloader as api
            # ä¸‹è½½è¾ƒå°çš„word2vecæ¨¡å‹
            model = api.load('word2vec-google-news-300')
            
            # ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ ¼å¼
            model.save_word2vec_format(self.model_path, binary=True)
            print(f"å°å‹æ¨¡å‹ä¸‹è½½å®Œæˆ: {self.model_path}")
            return True
            
        except Exception as e:
            print(f"ä¸‹è½½å°å‹æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def create_dummy_model(self):
        """åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„å°å‹æ¨¡å‹ä¾›æµ‹è¯•ä½¿ç”¨"""
        print("åˆ›å»ºè™šæ‹ŸWord2Vecæ¨¡å‹ä¾›æµ‹è¯•...")
        
        try:
            from gensim.models import Word2Vec
            import nltk
            from nltk.tokenize import word_tokenize
            
            # åˆ›å»ºä¸€äº›ç¤ºä¾‹å¥å­
            sentences = [
                "this is good delivery robot service",
                "this is bad delivery experience", 
                "delivery robot is great and amazing",
                "delivery service excellent wonderful",
                "poor delivery experience terrible",
                "amazing delivery robot fantastic",
                "terrible service quality horrible",
                "wonderful delivery experience perfect",
                "robot automation technology future",
                "sidewalk delivery autonomous vehicle",
                "safety concern pedestrian traffic",
                "job displacement unemployment worry",
                "convenient efficient fast delivery",
                "innovative technology advancement",
                "negative positive neutral sentiment"
            ]
            
            # åˆ†è¯
            tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]
            
            # è®­ç»ƒå°å‹Word2Vecæ¨¡å‹
            model = Word2Vec(tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4, epochs=10)
            
            # ä¿å­˜æ¨¡å‹
            model.wv.save_word2vec_format(self.model_path, binary=True)
            print(f"è™šæ‹Ÿæ¨¡å‹åˆ›å»ºå®Œæˆ: {self.model_path}")
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºè™šæ‹Ÿæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def download_model(self, method='auto'):
        """
        ä¸‹è½½Word2Vecæ¨¡å‹
        
        Args:
            method: ä¸‹è½½æ–¹æ³• ('google_drive', 'alternative', 'small', 'dummy', 'auto')
        """
        if self.check_model_exists():
            print(f"Word2Vecæ¨¡å‹å·²å­˜åœ¨: {self.model_path}")
            return True
        
        print("Word2Vecæ¨¡å‹ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        
        if method == 'auto':
            # è‡ªåŠ¨å°è¯•å„ç§æ–¹æ³•
            methods = ['google_drive', 'alternative', 'small', 'dummy']
        else:
            methods = [method]
        
        for method in methods:
            print(f"\nå°è¯•æ–¹æ³•: {method}")
            
            if method == 'google_drive':
                if self.download_from_google_drive():
                    return True
            elif method == 'alternative':
                if self.download_from_alternative_source():
                    return True
            elif method == 'small':
                if self.download_smaller_model():
                    return True
            elif method == 'dummy':
                if self.create_dummy_model():
                    return True
        
        print("æ‰€æœ‰ä¸‹è½½æ–¹æ³•éƒ½å¤±è´¥äº†")
        return False
    
    def load_model(self, fix_encoding=True):
        """åŠ è½½Word2Vecæ¨¡å‹"""
        if not self.check_model_exists():
            print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½")
            return None
        
        # é¦–å…ˆéªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        if not self.verify_model_integrity():
            print("æ¨¡å‹æ–‡ä»¶æŸåï¼Œå°è¯•é‡æ–°ä¸‹è½½...")
            if self.backup_and_redownload():
                # é‡æ–°ä¸‹è½½æˆåŠŸï¼Œç»§ç»­åŠ è½½
                pass
            else:
                print("é‡æ–°ä¸‹è½½å¤±è´¥")
                return None
        
        # å°è¯•æ ‡å‡†åŠ è½½
        try:
            print("æ­£åœ¨åŠ è½½Word2Vecæ¨¡å‹...")
            model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)
            print("Word2Vecæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
        except Exception as e:
            print(f"æ ‡å‡†åŠ è½½å¤±è´¥: {e}")
            
            # å¦‚æœå¯ç”¨ç¼–ç ä¿®å¤ï¼Œå°è¯•ä¿®å¤
            if fix_encoding:
                print("å°è¯•ä¿®å¤ç¼–ç é—®é¢˜...")
                model = self.fix_encoding_issues()
                if model is not None:
                    return model
                
                # å¦‚æœä¿®å¤å¤±è´¥ï¼Œå°è¯•é‡æ–°ä¸‹è½½å°å‹æ¨¡å‹
                print("ç¼–ç ä¿®å¤å¤±è´¥ï¼Œå°è¯•ä¸‹è½½å°å‹æ¨¡å‹...")
                if self.backup_and_redownload():
                    try:
                        model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)
                        print("é‡æ–°ä¸‹è½½ååŠ è½½æˆåŠŸ")
                        return model
                    except:
                        pass
                
                # æœ€åå°è¯•åˆ›å»ºè™šæ‹Ÿæ¨¡å‹
                print("åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ä½œä¸ºæœ€åæ‰‹æ®µ...")
                if self.create_dummy_model():
                    try:
                        model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)
                        print("è™šæ‹Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
                        return model
                    except:
                        pass
            
            print("æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†")
            return None
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.check_model_exists():
            return None
        
        try:
            model = self.load_model()
            if model is not None:
                vocab_size = len(model.key_to_index)
                vector_size = model.vector_size
                return {
                    'vocab_size': vocab_size,
                    'vector_size': vector_size,
                    'model_path': self.model_path,
                    'file_size': os.path.getsize(self.model_path)
                }
        except Exception as e:
            print(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        
        return None

def download_word2vec_model(model_dir='data', method='auto'):
    """ä¾¿æ·å‡½æ•°ï¼šä¸‹è½½Word2Vecæ¨¡å‹"""
    downloader = Word2VecDownloader(model_dir)
    return downloader.download_model(method)

def load_word2vec_model(model_dir='data', fix_encoding=True):
    """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½Word2Vecæ¨¡å‹"""
    downloader = Word2VecDownloader(model_dir)
    return downloader.load_model(fix_encoding=fix_encoding)

def fix_word2vec_encoding(model_dir='data'):
    """ä¾¿æ·å‡½æ•°ï¼šä¿®å¤Word2Vecç¼–ç é—®é¢˜"""
    downloader = Word2VecDownloader(model_dir)
    
    print("å¼€å§‹ä¿®å¤Word2Vecç¼–ç é—®é¢˜...")
    print("="*50)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not downloader.check_model_exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
    if not downloader.verify_model_integrity():
        print("âš ï¸ æ¨¡å‹æ–‡ä»¶æŸå")
        if downloader.backup_and_redownload():
            print("âœ… é‡æ–°ä¸‹è½½å®Œæˆ")
        else:
            print("âŒ é‡æ–°ä¸‹è½½å¤±è´¥")
            return False
    
    # å°è¯•åŠ è½½
    model = downloader.load_model(fix_encoding=True)
    if model is not None:
        print("ğŸ‰ Word2Vecç¼–ç é—®é¢˜ä¿®å¤æˆåŠŸ!")
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: è¯æ±‡é‡={len(model.key_to_index)}, å‘é‡ç»´åº¦={model.vector_size}")
        return True
    else:
        print("âŒ Word2Vecç¼–ç é—®é¢˜ä¿®å¤å¤±è´¥")
        return False 